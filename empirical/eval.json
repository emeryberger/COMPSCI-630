{ "title": "SIGPLAN Empirical Evaluation Checklist",
  "date": "March 2018",
  "credits": "E. D. Berger, S. M. Blackburn, M. Hauswirth, and M. Hicks for the ACM SIGPLAN EC",
  "version": "",
  "url": "http://www.sigplan.org/Resources/EmpiricalEvaluation/",
  "rationale": "rationale goes here",
  "groups": 
   [ { "name": "Clearly Stated Claims",
       "keyword": "Claims",
       "color": "Green",
       "items": 
        [ { "name": "Explicit Claims",
            "include": "yes",
            "figure": "figs/explicit_claims.jpg",
            "desc": "Claims must be explicit in order for the reader to assess whether the empirical evaluation supports them.  Claims should aim to state not just what is achieved but how." },
          { "name": "Appropriately-Scoped Claims",
            "include": "yes",
            "figure": "figs/appropriately-scoped_claims.jpg",
            "desc": "The truth of claims should follow from the evidence provided. Overclaiming is often the consequence of inadequate evidence, e.g., claiming \"works for all Java\", but evaluating only a static subset or claiming \"works on real hardware\", but evaluating only in (unrealistic) simulation.",
            "notes": "This includes implied generality (implied: \"works for all Java\", but actually only static subset; implied: \"works on real hardware\", but actually only works in simulation; implied: \"automatic process\", but in fact required non-trivial human supervision; implied: \"only improves the systems\" performance\", but actually the approach requires breaking some of the system\"s expected behavior) Also: Demonstrate on X, but unclear if relevant to target Y made in the claim" },
          { "name": "Threats to Validity of Claims",
            "include": "yes",
            "figure": "figs/threats_to_validity_of_claims.jpg",
            "desc": "A paper should state the most important threats to the validity of its claims, to place the scope of results in context. Stating no threats at all, or only tangential ones while omitting the more relevant ones, may mislead the reader to drawing too-strong conclusions." } ] },
     { "name": "Suitable Comparison",
       "keyword": "Comparison",
       "color": "CornflowerBlue",
       "items": 
        [ { "name": "Appropriate Baseline for Comparison",
            "include": "yes",
            "figure": "figs/appropriate_baseline_for_comparison.jpg",
            "desc": "An empirical evaluation of a contribution that improves upon the state-of-the-art should evaluate that contribution against an appropriate baseline, such as the current best-of-breed competitor or a randomized baseline.",
            "notes": "An evaluation of an idea that improves upon the state-of-the-art should evaluate that idea against a baseline. This baseline could be a best-of-breed competitor, but should not be a straw man, e.g., something that once was, but is no longer, the state-of-the-art. The baseline could also be an unsophisticated approach to the same problem, e.g., a fancy testing tool is usefully compared against one that is purely random, in order to see whether it does better." },
          { "name": "Fair Comparison",
            "include": "yes",
            "figure": "figs/fair_comparison.jpg",
            "desc": "Comparisons to a competing system should not unfairly disadvantage that system. For example, ideally, the compared systems would be compiled with the same compiler and optimization flags.",
            "notes": "For example, the authors were unable to build the state-of-the-art baseline at the -O3 optimization level and used -O0 instead, while using -O3 for their system." } ] },
     { "name": "Principled Benchmark Choice",
       "keyword": "Benchmark",
       "color": "LightPink",
       "items": 
        [ { "name": "Appropriate Suite",
            "include": "yes",
            "figure": "figs/appropriate_suite.jpg",
            "desc": "Evaluations should be conducted using the appropriate established benchmarks where they exist. Established suites should be used in the designed-for context; for example, it would be wrong to use a single-threaded suite for studying parallel performance.",
            "notes": "This includes misuse of incorrect established suite eg use of SPEC CINT2006 when considering parallel workloads" },
          { "name": "Non-Standard Suite(s) Justified",
            "include": "yes",
            "figure": "figs/non-standard_suites_justified.jpg",
            "desc": "Sometimes an established benchmark suite does not exist. A rationale should be provided for the selection of homegrown benchmarks or subsetting established benchmark suites.",
            "notes": "Note that \"benchmark\" here includes what is measured and the parameters of that measurement. One example of an oft-unappreciated benchmark parameter is timeout choice." },
          { "name": "Applications, Not (Just) Kernels",
            "include": "yes",
            "figure": "figs/applications_not_just_kernels.jpg",
            "desc": "A claim that a system benefits overall applications should be tested on such applications directly, and not only on micro-kernels (which can be useful and appropriate, in a broader evaluation)",
            "notes": "(and usually the claim does imply this)." } ] },
     { "name": "Adequate Data Analysis",
       "keyword": "Data Analysis",
       "color": "Gold",
       "items": 
        [ { "name": "Sufficient Number of Trials",
            "include": "yes",
            "figure": "figs/sufficient_number_of_trials.jpg",
            "desc": "In modern systems, which have non-deterministic performance, a small number of trials (e.g., a single time measurement) risks treating noise as signal. Similarly, more trials may be needed to get the system into an intended state (e.g., into a steady state that avoids warm-up effects).",
            "notes": ", and to assess the statistical significance of the difference between multiple means." },
          { "name": "Appropriate Summary Statistics",
            "include": "yes",
            "figure": "figs/appropriate_summary_statistics.jpg",
            "desc": "There are many summary statistics, and each presents an accurate view of a dataset only under appropriate circumstances. For example, the geometric mean should only be used when comparing values with different ranges, and the harmonic mean when comparing rates. When distributions have outliers, a median should be presented.",
            "notes": "The appropriate summary statistic (mean or median) should be used. For example, geometric mean should be used when comparing values with different ranges, and the harmonic mean when comparing rates. When distributions have outliers, a median should be presented." },
          { "name": "Report Data Distribution",
            "include": "yes",
            "figure": "figs/confidence_intervals.jpg",
            "desc": "Reporting just a measure of central tendency (e.g., a mean or median) fails to capture the extent of any non-determinism. A measure of variability (e.g., variance, std deviation, quantiles) and/or confidence intervals help to understand the distribution of the data." } ] },
     { "name": "Relevant Metrics",
       "keyword": "Metrics",
       "color": "MediumPurple",
       "items": 
        [ { "name": "Direct or Appropriate Proxy Metric",
            "include": "yes",
            "figure": "figs/direct_or_appropriate_proxy_metric.jpg",
            "desc": "If the most relevant evaluation metric is not (or cannot be) measured directly, the proxy metric used instead must be well justified. For example, a reduction in cache misses is not an appropriate proxy for actual end-to-end performance or energy consumption." },
          { "name": "Measures All Important Effects",
            "include": "yes",
            "figure": "figs/measure_all_important_effects.jpg",
            "desc": "The costs and benefits of a technique may be multi-faceted. All facets should be considered, both costs and benefits, and ideally evaluated. For example, compiler optimizations may speed up programs at the cost of drastically increasing compile times." } ] },
     { "name": "Appropriate and Clear Experimental Design",
       "keyword": "Experimental Design",
       "color": "SandyBrown",
       "items": 
        [ { "name": "Sufficient Information to Repeat",
            "include": "yes",
            "figure": "figs/sufficient_information_to_repeat.jpg",
            "desc": "Experiments should be described in sufficient detail to be repeatable. All parameters (including default values) should be included, as well as all version numbers of software, and full details of hardware platforms." },
          { "name": "Reasonable Platform",
            "include": "yes",
            "figure": "figs/reasonable_platform.jpg",
            "desc": "The evaluation should be on a platform that can reasonably be said to match the claims. For example, a claim that relates to performance on mobile platforms should not have an evaluation performed exclusively on server." },
          { "name": "Explores Key Design Parameters",
            "include": "yes",
            "figure": "figs/explores_key_design_parameters.jpg",
            "desc": "Key parameters should be explored over a range to evaluate sensitivity to their settings. Examples include the size of the heap when evaluating garbage collection and the size of caches when evaluating a locality optimization. All expected system configurations (e.g., from warmup to steady state) should be considered." },
          { "name": "Open Loop in Workload Generator",
            "include": "yes",
            "figure": "figs/open_loop_in_workload_generator.jpg",
            "desc": "Load generators for typical transaction-oriented systems should not be gated by the rate at which the system responds.  Rather, the load generator should be \"open loop\", generating work independent of the performance of the system under test.   See [Scrhoeder et al, 2006]",
            "notes": "Common error in networking and search, called out in a seminal paper by Scrhoeder et al in 2006 (https://dl.acm.org/citation.cfm?id=1267698)." },
          { "name": "Cross-Validation Where Needed",
            "include": "yes",
            "figure": "figs/cross-validation_where_needed.jpg",
            "desc": "When a system aims to be general but was developed by training on or close consideration of specific examples, it is essential that the evaluation explicitly perform cross-validation, so that the system is evaluated on data distinct from the training set.",
            "notes": "https://en.wikipedia.org/wiki/Cross-validation_(statistics)" } ] },
     { "name": "Appropriate Presentation of Results",
       "keyword": "Presentation",
       "color": "Orchid",
       "items": 
        [ { "name": "Comprehensive Summary Results",
            "include": "yes",
            "figure": "figs/comprehensive_summary_results.jpg",
            "desc": "Appropriate statistics should be used to characterize the full range of results, not just the most favorable values, which may be outliers.  For example, it is not appropriate to summarize speedups of 4%, 6%, 7%, and 49% as \"up to 49%\"." },
          { "name": "Axes Include Zero",
            "include": "yes",
            "figure": "figs/axes_include_zero.jpg",
            "desc": "A truncated graph (with an axis not including zero) can exaggerate the importance of a difference. While `zooming' in to the interesting range of an axis can sometimes aid exposition, there is a significant risk that this is misleading (especially if it is not immediately clear that the axis is truncated)." },
          { "name": "Ratios Plotted Correctly",
            "include": "yes",
            "figure": "figs/ratios_plotted_correctly.jpg",
            "desc": "When ratios (e.g. speedups) are plotted on one graph, the size of the bars must be linearly/logarithmically proportional to the change.  For example, 2.0 and 0.5 are reciprocals, but their linear distance from 1.0 does not reflect that. This misleading effect can be avoided either by using a log scale or by normalizing to the lowest (highest) value.",
            "notes": "For example, if times for a and b are 4 sec and 8 sec respectively for benchmark x and 6 sec and 3 sec for benchmark y, this could be shown as a/b (0.5, 2.0) or b/a (2.0, 0.5), where 1.0 represents parity. Although the results (0.5 & 2.0) are reciprocals, their distance from 1.0 on a linear scale is different by a factor of two (0.5 & 1.0), overstating the speedup. Therefore showing ratios (or percentages) greater than 1.0 (100%) and less than 1.0 (100%) on the same linear scale is visually misleading." },
          { "name": "Appropriate Level of Precision",
            "include": "yes",
            "figure": "figs/appropriate_level_of_precision.jpg",
            "desc": "The number of significant digits should reflect the precision of the experiment.   Reporting improvements of \"49.9%\" when the experimental error is +/- 1% is an example of mis-stated precision, misleading the reviewer\'s understanding of the significance of the rest." } ] } ] }
